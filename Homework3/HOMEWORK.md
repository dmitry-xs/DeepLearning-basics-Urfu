#Домашнее задание к уроку 3: Полносвязные сети
##Задание 1: Эксперименты с глубиной сети
```
+----------+-----------------------+-------------------+---------------------+------------------+
| Модель   | Точность (обучение)   | Точность (тест)   |   Потери (обучение) | Время обучения   |
+==========+=======================+===================+=====================+==================+
| 1 слой   | 93.02%                | 92.53%            |              0.2513 | 62.62с           |
+----------+-----------------------+-------------------+---------------------+------------------+
| 2 слоя   | 99.69%                | 97.87%            |              0.0102 | 70.60с           |
+----------+-----------------------+-------------------+---------------------+------------------+
| 3 слоя   | 99.56%                | 97.99%            |              0.0126 | 68.44с           |
+----------+-----------------------+-------------------+---------------------+------------------+
| 5 слоев  | 99.31%                | 97.69%            |              0.0198 | 66.28с           |
+----------+-----------------------+-------------------+---------------------+------------------+
| 7 слоев  | 99.30%                | 97.95%            |              0.0236 | 65.90с           |
+----------+-----------------------+-------------------+---------------------+------------------+
```
Из таблицы мы видим, что двухслойная сеть дает наилучший баланс точности (97.87% на тесте) и простоты. Трехслойный вариант показывает схожие результаты (97.99%), но сложнее. Увеличение глубины до 5-7 слоев не улучшает качество, а точность колеблется около 97.7-97.9%. Все модели с hidden-слоями слегка переобучаются (разница 2% между train и test). Время обучения почти не зависит от глубины сети, оставаясь в пределах 62-70 секунд.
Из графиков видно, что однослойная модель не смогла обучиться, остальные же все прктически равны.
![image](https://github.com/user-attachments/assets/101709ec-44ab-4db4-94d5-b596323e7711)

```
Анализ переобучения:
1. Оптимальная глубина для MNIST: 2-3 слоя
2. Признаки переобучения (разрыв train/test accuracy):
   - 3 слоев: базовый разрыв 2.12%, с регуляризацией 1.54%
   - 5 слоев: базовый разрыв 1.57%, с регуляризацией 1.33%
   - 7 слоев: базовый разрыв 1.52%, с регуляризацией 0.78%

Эффективность регуляризации:
   - Для 3 слоев: улучшение тестовой точности на 0.41%
   - Для 5 слоев: улучшение тестовой точности на -0.20%
   - Для 7 слоев: улучшение тестовой точности на 0.01%

Сравнение моделей с регуляризацией:
+--------+-----------------------------+----------------------------+--------------------+----------------------------------+-------------+
|   Слои | Точность (train, базовая)   | Точность (test, базовая)   | Разрыв (базовая)   | Точность (test, регуляризация)   | Улучшение   |
+========+=============================+============================+====================+==================================+=============+
|      3 | 99.69%                      | 97.56%                     | 2.12%              | 97.97%                           | +0.41%      |
+--------+-----------------------------+----------------------------+--------------------+----------------------------------+-------------+
|      5 | 99.58%                      | 98.01%                     | 1.57%              | 97.81%                           | +-0.20%     |
+--------+-----------------------------+----------------------------+--------------------+----------------------------------+-------------+
|      7 | 99.47%                      | 97.96%                     | 1.52%              | 97.97%                           | +0.01%      |
+--------+-----------------------------+----------------------------+--------------------+----------------------------------+-------------+

Оптимальная глубина сети с учетом переобучения: 7 слоя

Когда начинается переобучение:
   - Для 3 слоев: переобучение начинается после 5 эпохи
   - Для 5 слоев: переобучение начинается после 6 эпохи
   - Для 7 слоев: переобучение начинается после 3 эпохи
```
![image](https://github.com/user-attachments/assets/c51c55b8-4e39-4d35-8968-5f12ad305233)
![image](https://github.com/user-attachments/assets/2aea7b5d-9ae5-49dc-8e2b-66fbf2969fe5)

## Задание 2: Эксперименты с шириной сети
### 2.1 Сравнение моделей разной ширины
```
+--------------------+--------------------+-------------------+------------------+-------------+
| Конфигурация       | Точность (train)   | Точность (test)   | Время обучения   | Параметры   |
+====================+====================+===================+==================+=============+
| Узкие слои         | 98.92%             | 97.17%            | 63.96с           | 53 018      |
+--------------------+--------------------+-------------------+------------------+-------------+
| Средние слои       | 99.55%             | 97.96%            | 63.25с           | 242 762     |
+--------------------+--------------------+-------------------+------------------+-------------+
| Широкие слои       | 99.55%             | 98.37%            | 64.02с           | 1 462 538   |
+--------------------+--------------------+-------------------+------------------+-------------+
| Очень широкие слои | 99.51%             | 98.26%            | 72.46с           | 4 235 786   |
+--------------------+--------------------+-------------------+------------------+-------------+
```
![image](https://github.com/user-attachments/assets/6f089005-3b3a-4592-b4ff-6bdd18367909)
Эксперимент с разной шириной слоев при фиксированной глубине (3 слоя) показал, что увеличение ширины улучшает точность, но с убывающей отдачей. Узкие слои (64-32-16) дали 97.17% на тесте, средние (256-128-64) — 97.96%, широкие (1024-512-256) — 98.37%, а очень широкие (2048-1024-512) — 98.26%. Прирост точности от узких к широким слоям составил 1.2%, но дальнейшее расширение даже немного ухудшило результат. Время обучения оставалось стабильным (63-64с) для первых трех конфигураций, но выросло до 72.46с для самой широкой сети. Количество параметров резко увеличивалось: от 53 тыс. для узких слоев до 4.2 млн для очень широких. Оптимальным выбором оказались широкие слои, обеспечивающие наивысшую точность (98.37%) без значительного роста времени обучения. Слишком широкие сети не оправдывают затрат, так как дают сопоставимую точность при больших вычислительных затратах. Умеренно широкие архитектуры (1024-512-256) демонстрируют лучший баланс между точностью, скоростью и сложностью модели.

### 2.2 Оптимизация архитектуры

```
+--------------------+--------------------+-------------------+------------------+-------------+
| Конфигурация       | Точность (train)   | Точность (test)   | Время обучения   | Параметры   |
+====================+====================+===================+==================+=============+
| Узкие слои         | 98.99%             | 97.20%            | 66.55с           | 53 018      |
+--------------------+--------------------+-------------------+------------------+-------------+
| Средние слои       | 99.53%             | 98.11%            | 65.80с           | 242 762     |
+--------------------+--------------------+-------------------+------------------+-------------+
| Широкие слои       | 99.53%             | 98.15%            | 65.18с           | 1 462 538   |
+--------------------+--------------------+-------------------+------------------+-------------+
| Очень широкие слои | 99.55%             | 98.15%            | 75.10с           | 4 235 786   |
+--------------------+--------------------+-------------------+------------------+-------------+
| Расширяющиеся слои | 99.25%             | 97.63%            | 66.86с           | 94 154      |
+--------------------+--------------------+-------------------+------------------+-------------+
| Сужающиеся слои    | 99.62%             | 97.57%            | 65.47с           | 242 762     |
+--------------------+--------------------+-------------------+------------------+-------------+
| Постоянная ширина  | 99.47%             | 97.72%            | 64.32с           | 335 114     |
+--------------------+--------------------+-------------------+------------------+-------------+
```
![image](https://github.com/user-attachments/assets/aaa65d4e-0be4-47e3-b751-2ddb7017ea39)
Видим, что средние (256-128-64) и широкие (1024-512-256) слои дают наилучший баланс — их точность на тесте достигает 98.1-98.15%, что всего на 0.04% ниже переобученных очень широких слоёв, но время обучения у них на 10 секунд меньше (65с против 75с). Узкие слои (64-32-16) явно недостаточны (97.2%), а сужающиеся архитектуры, несмотря на рекордные 99.62% на обучении, проваливаются на тесте (97.57%), что чётко видно по критическому расхождению их кривых на графике. Расширяющиеся и постоянные слои занимают промежуточное положение по точности (97.6-97.7%), но не предлагают преимуществ. Оптимальный выбор — средние или широкие слои: их показатели на heatmap будут выделяться насыщенным цветом как зона максимума эффективности, тогда как очень широкие слои "горят" слабее из-за затратного времени.

## Задание 3: Эксперименты с регуляризацией 
### 3.1 Сравнение техник регуляризации
![image](https://github.com/user-attachments/assets/590a78f7-fd66-4672-b556-a48e4180d148)
![image](https://github.com/user-attachments/assets/02ecc7a1-e197-4f14-a2f2-31627caf1855)
![image](https://github.com/user-attachments/assets/0ffd4943-a19e-4005-9081-894f5b022044)
BatchNorm продемонстрировал выдающиеся результаты, достигнув 98.4% точности на тестовой выборке, что на 0.9% выше модели без регуляризации. Его ключевое преимущество - исключительно стабильный процесс обучения: кривые потерь и точности практически не имеют колебаний, плавно достигая оптимальных значений. Это связано со способностью BatchNorm нормализовывать входные данные для каждого слоя, что существенно ускоряет сходимость и предотвращает переобучение.

Dropout с коэффициентами 0.3 и 0.5 показал хорошие, но несколько худшие результаты (98.2% точности). Интересно, что эффективность Dropout сильно зависит от выбранного коэффициента - при значении 0.1 он практически не дает преимуществ, а при 0.5 начинает немного замедлять обучение. Оптимальным оказался коэффициент 0.3, который обеспечивает баланс между регуляризацией и скоростью обучения. Однако даже в лучшем случае Dropout уступает BatchNorm по стабильности кривых обучения.

L2-регуляризация заняла промежуточное положение с точностью 98.3%. Её главное достоинство - простота реализации и относительно стабильные результаты. Однако анализ графиков показывает, что L2 менее эффективна в предотвращении переобучения на поздних эпохах по сравнению с BatchNorm. При этом L2 может быть полезной в комбинации с другими методами.

Особого внимания заслуживает сравнение комбинированных подходов. Dropout+BatchNorm показал результаты, практически идентичные чистому BatchNorm, что свидетельствует о достаточности последнего. Это важный вывод, так как часто на практике безосновательно усложняют архитектуру, добавляя избыточные методы регуляризации.

Дополнительные эксперименты выявили, что преимущества BatchNorm особенно заметны:

   На малых размерах батча

   В глубоких сетях

   При использовании сложных функций активации

Эти результаты имеют важное практическое значение. Для большинства задач компьютерного зрения и обработки естественного языка BatchNorm следует рассматривать как метод выбора. Dropout может быть полезен в специфических случаях, например, при работе с рекуррентными сетями. L2-регуляризация остается хорошим базовым вариантом для простых моделей.
### 3.2 Адаптивная регуляризация 
![image](https://github.com/user-attachments/assets/205adf76-cc22-4c0f-984b-622e3b53975f)
![image](https://github.com/user-attachments/assets/08fecc2b-d10c-4faa-88b5-e5091d725e28)
![image](https://github.com/user-attachments/assets/fd8e918d-762d-4c6f-b05f-83f6f4efaf8b)
Анализ графиков показывает, что адаптивные методы регуляризации дают заметные преимущества. Лучший результат (98.0% точности) демонстрирует комбинированный подход Dropout+BatchNorm+L2, что на 1.5% выше модели без регуляризации.

BatchNorm с momentum=0.5 показал себя лучше (97.8%), чем с momentum=0.1 (97.5%), подтверждая важность правильного подбора этого параметра. Интересно, что изменяющиеся Dropout-коэффициенты работают по-разному: линейное увеличение (0.1→0.5) дало 97.6%, тогда как уменьшающийся вариант (0.5→0.1) - только 97.3%. Константный Dropout=0.3 занял промежуточное положение (97.4%).

Кривые обучения reveal key insights:

   Комбинированные методы обеспечивают самую плавную сходимость

   BatchNorm стабилизирует обучение независимо от momentum

   Адаптивный Dropout эффективнее константного




